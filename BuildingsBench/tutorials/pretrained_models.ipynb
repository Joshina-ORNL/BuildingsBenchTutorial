{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pretrained_models\n",
    "\n",
    "Download and run pretrained models on the BuildingsBench benchmark.\n",
    "\n",
    "This tutorial assumes you have followed the [installation instructions](https://nrel.github.io/BuildingsBench/#installation) for BuildingsBench, have [downloaded the datasets](https://nrel.github.io/BuildingsBench/datasets), and have set the `$BUILDINGS_BENCH` environment variable appropriately.\n",
    "\n",
    "## Step 1: Download a pretrained model from the OEDI\n",
    "\n",
    "From the base directory of the BuildingsBench code repository, run the following in the command line:\n",
    "\n",
    "```bash\n",
    "mkdir checkpoints\n",
    "cd checkpoints\n",
    "wget https://oedi-data-lake.s3.amazonaws.com/buildings-bench/v1.1.0/checkpoints/Transformer_Gaussian_S.pt\n",
    "```\n",
    "\n",
    "Other pretrained models are available at `https://oedi-data-lake.s3.amazonaws.com/buildings-bench/v1.1.0/checkpoints/`.\n",
    "\n",
    "## Step 1.5: Test that your model is working\n",
    "\n",
    "You should be able to run the following command from the base directory of the BuildingsBench code repository (assuming you are running on a GPU):\n",
    "\n",
    "```bash\n",
    "python3 scripts/zero_shot.py --model TransformerWithGaussian-S --checkpoint ./checkpoints/Transformer_Gaussian_S.pt\n",
    "```\n",
    "\n",
    "In what follows, we explain how to load and run the pretrained models in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables for this tutorial\n",
    "\n",
    "# Set this to your BuildingsBench github repo path\n",
    "repo_path = ''\n",
    "# This has to match the name of the .tomli file in the configs folder\n",
    "model_name = 'TransformerWithGaussian-S'\n",
    "# path to the checkpoint file you downloaded\n",
    "checkpoint = f'{repo_path}/checkpoints/Transformer_Gaussian_S.pt'\n",
    "# device, either 'cpu' or 'cuda:0'\n",
    "device = 'cuda:0'\n",
    "# dataset name\n",
    "dataset = 'electricity'\n",
    "# 'boxcox' if using the Gaussian model (continuous load values) else ''\n",
    "scaler_transform = 'boxcox'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadForecastingTransformer(\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (logits): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (power_embedding): Linear(in_features=1, out_features=64, bias=True)\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (building_embedding): Embedding(2, 32)\n",
       "  (lat_embedding): Linear(in_features=1, out_features=32, bias=True)\n",
       "  (lon_embedding): Linear(in_features=1, out_features=32, bias=True)\n",
       "  (day_of_year_encoding): TimeSeriesSinusoidalPeriodicEmbedding(\n",
       "    (linear): Linear(in_features=2, out_features=32, bias=True)\n",
       "  )\n",
       "  (day_of_week_encoding): TimeSeriesSinusoidalPeriodicEmbedding(\n",
       "    (linear): Linear(in_features=2, out_features=32, bias=True)\n",
       "  )\n",
       "  (hour_of_day_encoding): TimeSeriesSinusoidalPeriodicEmbedding(\n",
       "    (linear): Linear(in_features=2, out_features=32, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tomli\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path \n",
    "from buildings_bench.models import model_factory\n",
    "\n",
    "# Load the model arguments from the .tomli file\n",
    "config_path = Path(f'{repo_path}/buildings_bench/configs')\n",
    "if (config_path / f'{model_name}.toml').exists():\n",
    "    toml_args = tomli.load(( config_path / f'{model_name}.toml').open('rb'))\n",
    "    model_args = toml_args['model']\n",
    "else:\n",
    "    raise ValueError(f'Config {model_name}.toml not found.')\n",
    "\n",
    "# Create the model and move it to the device\n",
    "model, _, predict = model_factory(model_name, model_args)\n",
    "model = model.to(device)\n",
    "\n",
    "# Load from the checkpoint\n",
    "model.load_from_checkpoint(checkpoint)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load a building time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from buildings_bench import load_torch_dataset\n",
    "from buildings_bench.tokenizer import LoadQuantizer\n",
    "\n",
    "transform_path = Path(os.environ.get('BUILDINGS_BENCH', '')) \\\n",
    "    / 'metadata' / 'transforms'\n",
    "\n",
    "# Load the dataset generator\n",
    "buildings_datasets_generator = load_torch_dataset('electricity',\n",
    "                                                  apply_scaler_transform=scaler_transform,\n",
    "                                                  scaler_transform_path=transform_path)\n",
    "\n",
    "# the `building_dataset` is a torch.utils.data.Dataset object\n",
    "for building_id, building_dataset in buildings_datasets_generator:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Forward and inverse transforms for pre-processing the load time series\n",
    "\n",
    "The pretrained models expects the load time series to be transformed before being passed to the model.\n",
    "There are two main ways we pre-process the load time series values in BuildingsBench.\n",
    "\n",
    "1. The transformer trained with continuous load values and which predicts a Gaussian distribution per time step (e.g., `Transformer_Gaussian_S.pt`) uses a [Box-Cox transform](https://nrel.github.io/BuildingsBench/API/utilities/buildings_bench-transforms/#boxcoxtransform) to normalize the data. In our `buildings_bench` library, the forward Box-Cox transform is applied to the time series in the dataloader when creating a mini-batch. The inverse Box-Cox transform must be called manually on the predicted values to convert them back to the original scale (`buildings_bench.transforms.BoxCoxTransform.undo_transform`).\n",
    "2. The transformer trained with discrete load values and which predicts a categorical distribution per time step (e.g., `Transformer_Tokens_S.pt`) uses a KMeans [LoadQuantizer](https://nrel.github.io/BuildingsBench/API/utilities/buildings_bench-tokenizer/#tokenizer-quick-start) to tokenize the continuous load values into discrete values. In our `buildings_bench` library, the forward tokenization transform is applied directly before calling the model's forward pass, as we are able to run KMeans quantization on the GPU via `faiss-gpu` which vastly accelerates this step (`buildings_bench.tokenizer.LoadQuantizer.transform/undo_transform`).\n",
    "\n",
    "Here is our code for setting up the data transforms for the pretrained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scaler_transform == '':\n",
    "    # The tokenizer \n",
    "    load_transform = LoadQuantizer(\n",
    "        with_merge=True,\n",
    "        num_centroids=model.vocab_size,\n",
    "        device='cuda:0' if 'cuda' in device else 'cpu')\n",
    "    load_transform.load(transform_path)\n",
    "    # Grab the forward and inverse transform (tokenization) functions\n",
    "    transform = load_transform.transform\n",
    "    inverse_transform = load_transform.undo_transform\n",
    "elif scaler_transform != '': # continuous values\n",
    "    # the forward transform is handled by the Dataset \n",
    "    # so make this an identity function\n",
    "    transform = lambda x: x \n",
    "\n",
    "    if isinstance(building_dataset, torch.utils.data.ConcatDataset):\n",
    "        inverse_transform = building_dataset.datasets[0].load_transform.undo_transform\n",
    "    else:            \n",
    "        # Grab the undo transform function from the dataset object\n",
    "        inverse_transform = building_dataset.load_transform.undo_transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataloader for the building,\n",
    "# this will extract min(len(building_dataset),360) x [context_len + horizon] windows\n",
    "# in a sliding window fashion from the building\n",
    "building_dataloader = torch.utils.data.DataLoader(\n",
    "                        building_dataset,\n",
    "                        batch_size=360,\n",
    "                        shuffle=False)\n",
    "\n",
    "for batch in building_dataloader:\n",
    "\n",
    "    for k,v in batch.items():\n",
    "        batch[k] = v.to(device)\n",
    "\n",
    "    continuous_load = batch['load'].clone()\n",
    "    continuous_targets = continuous_load[:, model.context_len:]\n",
    "\n",
    "    # Apply forward transform\n",
    "    batch['load'] = transform(batch['load'])\n",
    "\n",
    "    # These could be tokens or continuous values\n",
    "    targets = batch['load'][:, model.context_len:]\n",
    "\n",
    "    # Call the model with the predict (for eval only)\n",
    "    if device == 'cuda:0':\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions, distribution_params = predict(batch)\n",
    "    else:\n",
    "        predictions, distribution_params = predict(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Inverse transform\n",
    "\n",
    "To get the predicted load time series back into the original scale, we need to apply the inverse transform to the predicted values. This is a bit complicated because the inverse transform is different for the two pretrained models and we might also want to invert the probabilistic parameters (e.g., the mean and variance of the Gaussian distribution) back into the original scale. The latter is unnecessary for the discrete transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either \"detokenizes\" the discrete load tokens back to continuous \n",
    "# values, or undoes the Box-Cox transform on the continuous values\n",
    "predictions = inverse_transform(predictions)\n",
    "\n",
    "# Backproject the Gaussian params to an approximate Gaussian in unscaled space\n",
    "# See our paper for details\n",
    "if scaler_transform == 'boxcox':   \n",
    "    mu = inverse_transform(distribution_params[:,:,0])\n",
    "    muplussigma = inverse_transform(torch.sum(distribution_params,-1))\n",
    "    sigma = muplussigma - mu\n",
    "    muminussigma = inverse_transform(distribution_params[:,:,0] - distribution_params[:,:,1])\n",
    "    sigma = (sigma + (mu - muminussigma)) / 2\n",
    "    distribution_params = torch.cat([mu.unsqueeze(-1), sigma.unsqueeze(-1)],-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2.1387],\n",
      "         [2.1055],\n",
      "         [2.4180],\n",
      "         ...,\n",
      "         [2.0273],\n",
      "         [1.8086],\n",
      "         [2.1543]],\n",
      "\n",
      "        [[2.1055],\n",
      "         [1.9512],\n",
      "         [2.3086],\n",
      "         ...,\n",
      "         [2.1543],\n",
      "         [2.1719],\n",
      "         [2.2383]],\n",
      "\n",
      "        [[2.1543],\n",
      "         [2.0586],\n",
      "         [2.0898],\n",
      "         ...,\n",
      "         [2.1875],\n",
      "         [2.2383],\n",
      "         [2.2910]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2.1055],\n",
      "         [2.2051],\n",
      "         [2.2383],\n",
      "         ...,\n",
      "         [1.4023],\n",
      "         [1.2637],\n",
      "         [1.5918]],\n",
      "\n",
      "        [[2.6738],\n",
      "         [2.5723],\n",
      "         [2.5117],\n",
      "         ...,\n",
      "         [2.4355],\n",
      "         [2.4551],\n",
      "         [2.5723]],\n",
      "\n",
      "        [[2.6934],\n",
      "         [2.2910],\n",
      "         [2.5117],\n",
      "         ...,\n",
      "         [2.4180],\n",
      "         [2.5117],\n",
      "         [2.6738]]], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
