{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b57bc3ab-7525-41b3-8d2f-fdaa2d593404",
   "metadata": {},
   "source": [
    "# Use MyModel-IDEAL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a5cfe292-a0a7-40a1-865f-bcb7bdda0d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tomli\n",
    "import matplotlib.pyplot as plt\n",
    "from os import environ\n",
    "from pathlib import Path\n",
    "from buildings_bench import load_torch_dataset\n",
    "from buildings_bench.models import model_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cdaeff6e-475d-460d-8bd3-aad9551fa2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "    def __init__(self): pass\n",
    "\n",
    "    def load_dataset(self, dataset_name, scaler_transform):\n",
    "        dataset = load_torch_dataset(\n",
    "            name=dataset_name,\n",
    "            apply_scaler_transform=scaler_transform,\n",
    "            scaler_transform_path=Path(environ[\"TRANSFORM_PATH\"])\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "    def create_dataloader(self, building_dataset):\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "                        building_dataset,\n",
    "                        batch_size=360,\n",
    "                        shuffle=False)\n",
    "        return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "333f0da1-1998-4ded-9b66-516d3b8434e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):    \n",
    "    def __init__(self,\n",
    "                 hidden_size=3,\n",
    "                 context_len=168,\n",
    "                 pred_len=24,\n",
    "                 continuous_loads=True,\n",
    "                 **args):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_len=context_len\n",
    "        self.hidden_size=hidden_size\n",
    "        self.pred_len=pred_len\n",
    "        self.continuous_loads=continuous_loads\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(context_len, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, pred_len*2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x['load'][:, :self.context_len, 0] # (batch_size, self.context_len)\n",
    "        out = self.mlp(x)  # (batch_size, self.pred_len*2)\n",
    "        return out.view(-1, self.pred_len, 2) # (batch_size, self.pred_len, 2)\n",
    "   \n",
    "\n",
    "    def predict(self, x):\n",
    "        out = self.forward(x)\n",
    "        means = out[:, :, 0].unsqueeze(2)\n",
    "        stds = nn.functional.softplus(out[:, :, 1].unsqueeze(2))\n",
    "        return means, torch.cat([means, stds], dim=2)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):    \n",
    "    def __init__(self,\n",
    "                 hidden_size=3,\n",
    "                 lstm_layers=1,\n",
    "                 context_len=168,\n",
    "                 pred_len=24,\n",
    "                 continuous_loads=True,\n",
    "                 **args):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_len = context_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pred_len = pred_len\n",
    "        self.continuous_loads = continuous_loads\n",
    "        self.lstm_layers = lstm_layers\n",
    "\n",
    "        self.power_embedding = nn.Linear(1, 64)\n",
    "        self.building_embedding = nn.Embedding(2, 32)\n",
    "        self.lat_embedding = nn.Linear(1, 32)\n",
    "        self.lon_embedding = nn.Linear(1, 32)\n",
    "\n",
    "        self.encoder = nn.LSTM(160, hidden_size, num_layers=lstm_layers, batch_first=True)\n",
    "        self.decoder = nn.LSTM(160, hidden_size, num_layers=lstm_layers, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)  # Final prediction\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding input features\n",
    "        lat = self.lat_embedding(x['latitude'])                  # [B, T, 32]\n",
    "        lon = self.lon_embedding(x['longitude'])                 # [B, T, 32]\n",
    "        btype = self.building_embedding(x['building_type'].squeeze(-1))  # [B, T, 32]\n",
    "        load = self.power_embedding(x['load'])     # [B, T, 64]\n",
    "\n",
    "        time_series_inputs = [lat, lon, btype, load]\n",
    "\n",
    "        # Concatenate all embeddings\n",
    "        time_series_embed = torch.cat(time_series_inputs, dim=2)  # [B, T, 256]\n",
    "\n",
    "        # Encode context sequence\n",
    "        context_seq = time_series_embed[:, :self.context_len, :]  # [B, 168, 256]\n",
    "        _, (h_n, c_n) = self.encoder(context_seq)  # hidden and cell states for decoder\n",
    "\n",
    "        # Prepare decoder input (repeat last context embedding)\n",
    "        decoder_input = time_series_embed[:, self.context_len - 1:self.context_len, :].repeat(1, self.pred_len, 1)\n",
    "\n",
    "        # Decode\n",
    "        decoder_out, _ = self.decoder(decoder_input, (h_n, c_n))  # [B, 24, hidden_size]\n",
    "        \n",
    "        return decoder_out\n",
    "\n",
    "    def predict(self, x):\n",
    "        out = self.forward(x)\n",
    "        output = self.output_layer(out).squeeze(-1)  # [B, 24]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b008ae48-45f5-4747-860f-65611bb09de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_name, train_dataset, scaler_transform, device, optimizer='adam', loss='mse'):\n",
    "    handler = DataHandler()\n",
    "    dataset = handler.load_dataset(train_dataset, scaler_transform)    \n",
    "\n",
    "    if model_name == 'MLP':\n",
    "        model = MLP().to(device)\n",
    "    elif model_name == 'RNN':\n",
    "        model = RNN().to(device)\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(10):\n",
    "        for building_id, building_dataset in dataset:\n",
    "            inverse_transform = building_dataset.datasets[0].load_transform.undo_transform\n",
    "            dataloader = handler.create_dataloader(building_dataset)\n",
    "            \n",
    "            for batch in dataloader:\n",
    "                for key, value in batch.items():\n",
    "                    batch[key] = value.to(device)\n",
    "                    \n",
    "                optimizer.zero_grad()\n",
    "                predictions = model(batch)\n",
    "                targets = batch['load'][:, 168:, 0]  # ground truth\n",
    "                loss = loss_fn(predictions[:, :, 0], targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "      \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "74e6f1db-cf47-49ac-9e0e-7561376b1bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, model_name, test_dataset, scaler_transform, device):\n",
    "    handler = DataHandler()\n",
    "    dataset = handler.load_dataset(test_dataset, scaler_transform)    \n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    target_list = []\n",
    "    prediction_list = []\n",
    "\n",
    "    model.eval()\n",
    "    for building_id, building_dataset in dataset:\n",
    "            \n",
    "        inverse_transform = building_dataset.datasets[0].load_transform.undo_transform\n",
    "        dataloader = handler.create_dataloader(building_dataset)\n",
    "            \n",
    "        for batch in dataloader:\n",
    "            for key, value in batch.items():\n",
    "                batch[key] = value.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                predictions = model.predict(batch)\n",
    "  \n",
    "        targets = batch['load'][:, 168:]\n",
    "        targets = inverse_transform(targets)\n",
    "        target_list.append(targets.detach().cpu())\n",
    "\n",
    "        if model_name = 'RNN':\n",
    "            predictions = inverse_transform(predictions)\n",
    "        if model_name = 'MLP'\n",
    "            predictions = inverse_transform(predictions[0])\n",
    "        prediction_list.append(predictions.detach().cpu())\n",
    "\n",
    "    prediction_list = torch.cat(prediction_list)\n",
    "    target_list = torch.cat(target_list)\n",
    "    \n",
    "    return prediction_list, target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "270b195e-de2d-4672-8442-a98d9a0767ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model_name, train_dataset, test_dataset, scaler_transform, device):\n",
    "    \n",
    "    model = train(model_name=model_name, train_dataset=train_dataset, scaler_transform=scaler_transform, device=device, optimizer='adam', loss='mse')\n",
    "    \n",
    "    test_dataset = 'sceaux'\n",
    "    preds, targets = test(model=model, model_name=model_name, test_dataset=test_dataset, scaler_transform=scaler_transform, device=device)\n",
    "    plt.plot(preds[0].squeeze(), label=\"Prediction\")\n",
    "    plt.plot(targets[0].squeeze(), label=\"Target\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    test_dataset = 'electricity'\n",
    "    preds, targets = test(model=model, model_name=model_name, test_dataset=test_dataset, scaler_transform=scaler_transform, device=device)\n",
    "    plt.plot(preds[0].squeeze(), label=\"Prediction\")\n",
    "    plt.plot(targets[0].squeeze(), label=\"Target\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    test_dataset = 'smart'\n",
    "    preds, targets = test(model=model, model_name=model_name, test_dataset=test_dataset, scaler_transform=scaler_transform, device=device)\n",
    "    plt.plot(preds[0].squeeze(), label=\"Prediction\")\n",
    "    plt.plot(targets[0].squeeze(), label=\"Target\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c7f9e4bf-d884-480d-a774-2fe2ff9e58c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Ideal | SMART\n",
    "    # Ideal | electricity\n",
    "    # Ideal | sceaux\n",
    "    train_dataset = 'ideal' # Electricity | SMART | Ideal | lcl \n",
    "    device = 'cuda:0'\n",
    "    model_name = 'RNN' # MLP\n",
    "    scaler_transform = 'boxcox'\n",
    "\n",
    "    PATH = '/pscratch/sd/n/nrushad'\n",
    "    environ[\"PATH\"] = PATH\n",
    "    environ[\"REPO_PATH\"] = f'{PATH}/BuildingsBenchTutorial/BuildingsBench/'\n",
    "    environ[\"BUILDINGS_BENCH\"] = f'{PATH}/Dataset'\n",
    "    environ[\"TRANSFORM_PATH\"] = f'{PATH}/Dataset/metadata/transforms'\n",
    "\n",
    "    model = main(model_name=model_name, train_dataset=train_dataset, test_dataset=test_dataset, scaler_transform=scaler_transform, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BuildingsBenchKernel",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
