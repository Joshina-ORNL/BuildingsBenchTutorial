{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ac47b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/n/nrushad/.conda/envs/BuildingsBenchEnv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from buildings_bench import load_torch_dataset\n",
    "from buildings_bench.models import model_factory\n",
    "\n",
    "import tomli\n",
    "from pathlib import Path\n",
    "import os \n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e715fda-612f-45ef-aa4e-8a953b5a2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data Handler ===\n",
    "class DataHandler:\n",
    "    def __init__(self, batch_size=32):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def load_dataset(self, dataset_name, scaler_transform):\n",
    "        from buildings_bench import load_torch_dataset\n",
    "        return list(load_torch_dataset(\n",
    "            dataset_name,\n",
    "            apply_scaler_transform=scaler_transform,\n",
    "            scaler_transform_path=Path(os.environ[\"TRANSFORM_PATH\"])\n",
    "        ))\n",
    "\n",
    "    def create_dataloader(self, dataset):\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eef5e153-8f21-4e12-aee9-6994d0308f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesSinusoidalPeriodicEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, embedding_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"`x` is expected to be [batch_size, seqlen, 1] in [-1, +1] range.\"\"\"\n",
    "        x = torch.cat([torch.sin(torch.pi * x), torch.cos(torch.pi * x)], dim=2)\n",
    "        return self.linear(x)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    DEFAULT_CONTEXT_LEN = 168\n",
    "    DEFAULT_PRED_LEN = 24\n",
    "\n",
    "    def __init__(self, activation):\n",
    "        super().__init__()\n",
    "        self.context_len = self.DEFAULT_CONTEXT_LEN\n",
    "        self.pred_len = self.DEFAULT_PRED_LEN\n",
    "        self.activation = self._get_activation(activation)\n",
    "        self.embeddings = self._create_embeddings()\n",
    "\n",
    "    def _create_embeddings(self):\n",
    "        return nn.ModuleDict({\n",
    "            'power': nn.Linear(1, 64),\n",
    "            'building': nn.Embedding(2, 32),\n",
    "            'lat': nn.Linear(1, 32),\n",
    "            'lon': nn.Linear(1, 32), \n",
    "            'day_of_year': TimeSeriesSinusoidalPeriodicEmbedding(32),\n",
    "            'day_of_week': TimeSeriesSinusoidalPeriodicEmbedding(32),\n",
    "            'hour_of_day': TimeSeriesSinusoidalPeriodicEmbedding(32)\n",
    "        })\n",
    "\n",
    "    def _get_activation(self, name):\n",
    "        return {\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"tanh\": nn.Tanh(),\n",
    "            \"gelu\": nn.GELU(),\n",
    "            \"leaky_relu\": nn.LeakyReLU()\n",
    "        }.get(name.lower(), nn.ReLU())\n",
    "\n",
    "    def _data_pre_process(self, x):\n",
    "        lat = self.embeddings['lat'](x['latitude'])\n",
    "        lon = self.embeddings['lon'](x['longitude'])\n",
    "        btype = self.embeddings['building'](x['building_type'].squeeze(-1))\n",
    "        load = self.embeddings['power'](x['load'])\n",
    "        day_of_year = self.embeddings['day_of_year'](x['day_of_year'])            \n",
    "        day_of_week = self.embeddings['day_of_week'](x['day_of_week'])            \n",
    "        hour_of_day = self.embeddings['hour_of_day'](x['hour_of_day']) \n",
    "        return torch.cat([lat, lon, btype, day_of_year, day_of_week, hour_of_day, load], dim=2)\n",
    "\n",
    "class NN(Model):\n",
    "    def __init__(self, activation):\n",
    "        super().__init__(activation)\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        input_dim = self.context_len * 256\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(input_dim, 128), \n",
    "            self.activation,\n",
    "            nn.Linear(128, self.pred_len)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        x_flat = ts_embed[:, :self.context_len, :].reshape(x['load'].shape[0], -1)\n",
    "        return self.model(x_flat).unsqueeze(-1)\n",
    "\n",
    "class MyNN(Model):\n",
    "    def __init__(self, activation):\n",
    "        super().__init__(activation)\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        input_dim = self.context_len * 256\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(input_dim, 512), self.activation,\n",
    "            # TODO #\n",
    "            # --- #\n",
    "            nn.Linear(512, self.pred_len) # Update\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        x_flat = ts_embed[:, :self.context_len, :].reshape(x['load'].shape[0], -1)\n",
    "        return self.model(x_flat).unsqueeze(-1)\n",
    "\n",
    "\n",
    "class RNN(Model):\n",
    "    def __init__(self, activation=\"relu\"):\n",
    "        super().__init__(activation)\n",
    "        self.rnn1, self.rnn2, self.output_layer = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        rnn1 = nn.RNN(256, 128, batch_first=True)\n",
    "        rnn2 = nn.RNN(128, 128, batch_first=True)\n",
    "        output_layer = nn.Linear(128, self.pred_len)\n",
    "        return rnn1, rnn2, output_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        out1, _ = self.rnn1(ts_embed)\n",
    "        out2, _ = self.rnn2(out1)\n",
    "        last_hidden = self.activation(out2[:, -1, :])\n",
    "        return self.output_layer(last_hidden).unsqueeze(-1)\n",
    "\n",
    "class LSTM(Model):\n",
    "    def __init__(self, activation=\"relu\"):\n",
    "        super().__init__(activation)\n",
    "        self.lstm1, self.lstm2, self.output_layer = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        lstm1 = nn.LSTM(256, 128, batch_first=True)\n",
    "        lstm2 = nn.LSTM(128, 128, batch_first=True)\n",
    "        output_layer = nn.Linear(128, self.pred_len)\n",
    "        return lstm1, lstm2, output_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        out1, _ = self.lstm1(ts_embed)\n",
    "        out2, _ = self.lstm2(out1)\n",
    "        last_hidden = self.activation(out2[:, -1, :])\n",
    "        return self.output_layer(last_hidden).unsqueeze(-1)\n",
    "\n",
    "class GRU(Model):\n",
    "    def __init__(self, activation=\"relu\"):\n",
    "        super().__init__(activation)\n",
    "        self.gru1, self.gru2, self.output_layer = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        gru1 = nn.GRU(256, 128, batch_first=True)\n",
    "        gru2 = nn.GRU(128, 128, batch_first=True)\n",
    "        output_layer = nn.Linear(128, self.pred_len)\n",
    "        return gru1, gru2, output_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        out1, _ = self.gru1(ts_embed)\n",
    "        out2, _ = self.gru2(out1)\n",
    "        last_hidden = self.activation(out2[:, -1, :])\n",
    "        return self.output_layer(last_hidden).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3099190-8a3c-4527-b467-8aec1afb8677",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model_name, device, scaler_transform, dataset_name, epochs, train_buildings, test_buildings, activation='relu', optimizer_name='adam', lr=1e-3):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.scaler_transform = scaler_transform\n",
    "        self.dataset_name = dataset_name\n",
    "        self.epochs = epochs\n",
    "        self.train_buildings = train_buildings\n",
    "        self.test_buildings = test_buildings\n",
    "        self.activation = activation\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.lr = lr\n",
    "        self.model = self._load_model()\n",
    "        self.optimizer = self._get_optimizer()\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.handler = DataHandler(batch_size=32)\n",
    "        self.path = os.path.join(os.getcwd(), dataset_name, model_name, activation, optimizer_name, f'epochs-{epochs}')\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "\n",
    "    def _load_model(self):\n",
    "        model_map = {\n",
    "            'NN': NN,\n",
    "            'RNN': RNN,\n",
    "            'LSTM': LSTM,\n",
    "            'GRU': GRU, \n",
    "            'MyNN': MyNN\n",
    "        }\n",
    "        return model_map[self.model_name](activation=self.activation).to(self.device)\n",
    "\n",
    "    def _get_optimizer(self):\n",
    "        opt_map = {\n",
    "            'adam': torch.optim.Adam,\n",
    "            'sgd': torch.optim.SGD,\n",
    "            'adamw': torch.optim.AdamW\n",
    "        }\n",
    "        optimizer_cls = opt_map.get(self.optimizer_name.lower(), torch.optim.Adam)\n",
    "        return optimizer_cls(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        log = []\n",
    "        start_time = time.time()  # Start timer\n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0.0\n",
    "            for building_id, building_dataset in self.train_buildings:\n",
    "                dataloader = self.handler.create_dataloader(building_dataset)\n",
    "                for batch in dataloader:\n",
    "                    for key, value in batch.items():\n",
    "                        batch[key] = value.to(self.device)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    predictions = self.model(batch)\n",
    "                    targets = batch['load'][:, self.model.context_len:, 0]\n",
    "                    loss = self.loss_fn(predictions[:, :, 0], targets)\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "            print(f\"[{self.model_name}] Epoch {epoch + 1}: Loss = {total_loss:.4f}\")\n",
    "            log.append({\"epoch\": epoch + 1, \"loss\": total_loss})\n",
    "        train_duration = time.time() - start_time  # End timer\n",
    "        with open(os.path.join(self.path, \"train_loss.json\"), \"w\") as f:\n",
    "             json.dump({\"train_loss\": log, \"train_duration\": train_duration}, f, indent=2)\n",
    "        return train_duration\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        results = {}\n",
    "        mae_total = 0.0\n",
    "        rmse_total = 0.0\n",
    "        r2_total = 0.0\n",
    "        count = 0\n",
    "        for building_id, building_dataset in self.test_buildings:\n",
    "            inverse_transform = building_dataset.datasets[0].load_transform.undo_transform\n",
    "            dataloader = self.handler.create_dataloader(building_dataset)\n",
    "            \n",
    "            target_list = []\n",
    "            prediction_list = []\n",
    "            load_list = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in dataloader:\n",
    "                    for key, value in batch.items():\n",
    "                        batch[key] = value.to(self.device)\n",
    "\n",
    "                    \n",
    "                    predictions = self.model(batch)\n",
    "                    targets = batch['load'][:, self.model.context_len:]\n",
    "                    loads = batch['load'][:, :self.model.context_len]\n",
    "                    \n",
    "                    targets = inverse_transform(targets)\n",
    "                    predictions = inverse_transform(predictions)\n",
    "                    loads = inverse_transform(loads)\n",
    "                    \n",
    "                    prediction_list.append(predictions.detach().cpu())\n",
    "                    target_list.append(targets.detach().cpu())\n",
    "                    load_list.append(loads.detach().cpu())\n",
    "            \n",
    "            predictions_all = torch.cat(prediction_list)\n",
    "            targets_all = torch.cat(target_list)\n",
    "            load_all = torch.cat(load_list)\n",
    "            \n",
    "            mae = torch.abs(predictions_all - targets_all).mean().item()\n",
    "            rmse = torch.sqrt(((predictions_all - targets_all) ** 2).mean()).item()\n",
    "            r2 = 1 - (((predictions_all - targets_all) ** 2).sum() / ((targets_all - targets_all.mean()) ** 2).sum()).item()\n",
    "            mae_total += mae\n",
    "            rmse_total += rmse\n",
    "            r2_total += r2\n",
    "            count += 1\n",
    "            # Convert tensors to lists before saving\n",
    "            results[building_id] = {\n",
    "                \"load\": load_all.tolist(),\n",
    "                \"predictions\": predictions_all.tolist(),\n",
    "                \"targets\": targets_all.tolist()\n",
    "            }\n",
    "        # Save prediction results\n",
    "        with open(os.path.join(self.path, \"predictions.json\"), \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        # Save evaluation metrics\n",
    "        eval_metrics = {\n",
    "            \"mae\": mae_total / count,\n",
    "            \"rmse\": rmse_total / count,\n",
    "            \"r2\": r2_total / count}\n",
    "        with open(os.path.join(self.path, \"evaluate_model.json\"), \"w\") as f:\n",
    "            json.dump(eval_metrics, f, indent=2)\n",
    "        return results, eval_metrics[\"mae\"], eval_metrics[\"rmse\"], eval_metrics[\"r2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44f2a80a-1232-4e76-88b4-520cc0c48e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset: ideal ===\n",
      "\n",
      "--- Training NN | Activation: relu | Optimizer: adam | Epochs: 2 ---\n",
      "[NN] Epoch 1: Loss = 3079.2469\n",
      "[NN] Epoch 2: Loss = 747.7447\n",
      "[NN] MAE: 0.2577, RMSE: 0.4031, R²: -0.2531\n",
      "Training Time: 94.38 seconds\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Search space\n",
    "# dataset_names = [\"ideal\", \"electricity\", \"lcl\", \"sceaux\", \"borealis\"]\n",
    "# model_classes = [\"NN\", \"RNN\", \"LSTM\", \"GRU\", \"MyNN\"]\n",
    "# activations = [\"relu\", \"tanh\", \"leaky_relu\", \"gelu\"]\n",
    "# optimizers = [\"adam\", \"sgd\", \"adamw\"]\n",
    "# epoch_options = [10, 20, 30]\n",
    "\n",
    "dataset_names = [\"ideal\"]\n",
    "model_classes = [\"NN\"]\n",
    "activations = [\"relu\"]\n",
    "optimizers = [\"adam\"]\n",
    "epoch_options = [2]\n",
    "\n",
    "\n",
    "# Set environment variables (same for all runs)\n",
    "base_path = \"/pscratch/sd/n/nrushad\"\n",
    "os.environ[\"PATH\"] = base_path\n",
    "os.environ[\"REPO_PATH\"] = f\"{base_path}/BuildingsBenchTutorial/BuildingsBench/\"\n",
    "os.environ[\"BUILDINGS_BENCH\"] = f\"{base_path}/Dataset\"\n",
    "os.environ[\"TRANSFORM_PATH\"] = f\"{base_path}/Dataset/metadata/transforms\"\n",
    "\n",
    "# Run all combinations\n",
    "for dataset_name in dataset_names:\n",
    "    print(f\"\\n=== Dataset: {dataset_name} ===\")\n",
    "    # Load and split dataset\n",
    "    handler = DataHandler(batch_size=32)\n",
    "    all_buildings = handler.load_dataset(dataset_name, scaler_transform=\"boxcox\")\n",
    "    train_buildings = all_buildings[:int(0.8 * len(all_buildings))]\n",
    "    test_buildings = all_buildings[int(0.8 * len(all_buildings)):]\n",
    "    for model_class in model_classes:\n",
    "        for activation in activations:\n",
    "            for optimizer_name in optimizers:\n",
    "                for epochs in epoch_options:\n",
    "                    print(f\"\\n--- Training {model_class} | Activation: {activation} | Optimizer: {optimizer_name} | Epochs: {epochs} ---\")\n",
    "                    trainer = Trainer(\n",
    "                        model_name=model_class,\n",
    "                        device=device,\n",
    "                        dataset_name=dataset_name,\n",
    "                        epochs=epochs,\n",
    "                        train_buildings=train_buildings,\n",
    "                        test_buildings=test_buildings,\n",
    "                        scaler_transform=\"boxcox\",\n",
    "                        activation=activation,\n",
    "                        optimizer_name=optimizer_name,\n",
    "                        lr=1e-3)\n",
    "                    train_duration = trainer.train()\n",
    "                    results, mae, rmse, r2 = trainer.evaluate()\n",
    "                    print(f\"[{model_class}] MAE: {mae:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "                    print(f\"Training Time: {train_duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea9525-3fb2-4c7c-9ef4-989c2542774f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BuildingsBenchKernel",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
